\documentclass[11pt,a4wide,titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{lineno}
\usepackage[a4paper]{geometry}
\usepackage{csvsimple}
\usepackage[nottoc]{tocbibind}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{graphicx}
\bibliographystyle{plainnat}
\onehalfspacing

\begin{document}
\begin{titlepage}
	\begin{center}
		\vspace*{1cm}
		
		\begin{Large}
			\textbf{Phenomenological Models Outperform Mechanistic Models\\
				for Estimation of Messy Biological Data}
		\end{Large}
		
		
		\vspace{1.5cm}
		
		\textbf{Eamonn Murphy}
		
		\vfill
		
		Department of Life Sciences\\
		Silwood Park\\
		Imperial College London\\
		\vspace{\baselineskip}
		Friday 3rd December 2021\\
		\vspace{\baselineskip}
		Word Count: 
		\vfill
		
	\end{center}
\end{titlepage}

\section*{Introduction}
Modelling population growth in bacteria is an important problem, as it allows for many applications, including prediction of bacterial numbers in food, which is important for spoilage. As well as prediction, population growth models can help to improve our mechanistic understanding of bacterial growth. This problem has been very well covered in the literature, and many data-sets of bacterial population growth are available. A large number of models have been proposed for bacterial growth, including a number of linear and non-linear models. In this mini-project, I have attempted to fit a number of phenomenological and mechanistic models to a provided data-set of bacterial growth, taken from 10 different published papers. These data include a variety of species, growth mediums and temperatures.

There are a number of important proposed parameters which help to explain the growth of a bacterial population. The first is that, upon introduction to a new environment, population size tends to follow a sigmoidal (or S-shaped) curve. This happens as bacteria must first adapt to their new environment before beginning to grow and expand. These adaptations can include up-regulating new sets of genes (such as the lac operon when they go from a glucose to lactose environment). The parameters that explain the shape of this curve include the initial population size ($N0$), the length of the lag phase (modelled by $t_{lag}$), the highest rate of population growth reached ($r_{max}$) and the carrying capacity  of the environment($K$). The carrying capacity is the maximum possible population that can be sustained in that environment, and explains underlying variables such as the nutrient density and type, the size of the environment, competition, and also other underlying characteristics of the bacterial species that you are studying.

The simplest set of models which can explain bacterial growth are linear quadratic and cubic models. These are what would normally be referred to as phenomenological models; they do not attempt to apply any of our knowledge about how bacterial populations grow and what determines it, and simply try to find a best fit to the existing data. However, there is some form of mechanistic explanation to any reasonably fitting model; for example, the intercept $B_0$ will likely be an approximation of the starting population size, while the slope $B_1$ will be a rough approximation of the rate of growth during the exponential phase.

\begin{center}
	Table 1: Equations for the Bacterial Growth Models
	\def\arraystretch{2}
	\begin{tabular}{c|c|c}
		Model & Formula & $k$ (number of parameters)\\
		\hline
		Quadratic & $N_t = B_2t^2 + B_1t + B_0$ & 3\\
		Cubic & $N_t = B_3t^3 + B_2t^2 + B_1t + B_0$ & 4\\
		Logistic & $N_t = \frac{N_0Ke^{rt}}{K + N_0(e^{rt} - 1)}$ & 3\\[2ex]
		Gompertz & $\ln(N_t) = N_0 + (K - N_0)e^{-e^{re\frac{t_{lag} - t}{(K - N_0)\ln(10)} + 1}}$ & 4\\[2ex]
	\end{tabular}
\end{center}
\textit{$t$ - time; $N_t$ - population size at time t; $K$ - carrying capacity; $r$ - highest rate of population growth reached; $t_{lag}$ - delay until population growth, or x-axis intercept of the tangent $r$}


There are also quite a large number of non-linear mechanistic models which have been developed to try to explain bacterial growth. One of the simplest is the logistic equation, which takes the form
\[N_t = \frac{N_0Ke^{rt}}{K + N_0(e^{rt} - 1)}\]
where $N_0$ is initial population size, $K$ is carrying capacity and $r$ is the maximum rate of population growth.

\section*{Methods}
The population growth data was obtained from 10 different papers \citep{baeGrowthCharacteristicsBiofilm2014a,bernhardtMetabolicTheoryTemperatureSize2018,galarzPredictingBacterialGrowth2016,gillGrowthEscherichiaColi1991,phillipsRelationTemperatureGrowth1987,rothContinuityPsychrophilicMesophilic1962,silvaModellingGrowthLactic2018,sivonenEffectsLightTemperature1990,stannardTemperatureGrowthRelationships1985,zwieteringModelingBacterialGrowth1994}. The analysis on the resulting dataset was carried out using 3 scripting languages, Python, R and Bash. Initial data visualisation and wrangling was carried out using Python. Following this, further data wrangling was carried out in R, and model fitting for the different proposed models was carried out.\newline

\begin{center}
	Table 1: Format of Columns in Dataset \newline
	\csvreader[tabular=|c|c|,
	table head = \hline,
	late after line=\\\hline]%
	{../data/LogisticGrowthMetaData.csv}{}{\csvlinetotablerow}
	\newline
\end{center}

\subsection*{Data Cleaning and Preparation}
A number of important choices were made when cleaning the data. First, a unique ID was assigned to each combination of citation, temperature and species, in order to identify separately each population / experiment. Following this, the subsets containing negative population values were removed from consideration, as I assumed these must have been measurement errors (since they are biologically impossible), and thus all of the population size measurements in that dataset could be suspect. I further cleaned the data by removing subsets with $\le 6$ data points. Given that the models have up to 4 parameters, models fit on data with only an equal or slightly greater number of data points will be overfit to random fluctuations in the data. Many of the other subsets will also encounter this problem given the sample sizes, but given that 41\% of the populations have less than 10 samples, I decided to draw the cutoff at 6. I standardised the time in each subset so that the smallest time value was always equal to zero and other times were proportional to that. Negative values for time are plausible, as they may have started the clock at, for example, -24 hours upon inoculation.

One of the papers \citep{bernhardtMetabolicTheoryTemperatureSize2018} included experimental replicates. I first tested that the replicates were not different from each other. In order to do this, I fit a simple quadratic regression model on time with replicate as an extra term. No significant differences were found (range of p $0.49 - 0.94$). Therefore, I decided to include the data as single subsets, discarding the replicate information from further modelling. Many of the models should be fit on the natural log of the population size. However, there were some population size values of 0 in the dataset. To deal with this, a small positive constant was added to all population values in the affected subsets, which is a common method as outlined in \cite{bellegoDealingLogsZeros2021}.

\subsection*{Model Fitting}
Quadratic and cubic models were fit using standard linear least squares fitting in R (i.e. minimising the sum of the squares). I fit the Logistic and Gompertz models using non-linear least squares, and start values were needed for the free parameters. These were estimated as follows for the Logistic model:
\begin{center}
\begin{tabular}{cc}
	$N_0$ & Minimum population size in subset\\
	$K$ & $2\times$ maximum population size in subset\\
	$r_{max}$ & Iteratively determined preset value\\
\end{tabular}
\end{center}

\subsection*{Software and Packages}
\textbf{Software Versions}
\begin{itemize}
    \item Ubuntu - 20.04.3 LTS
    \item Python - 3.8.10
    \item R - 4.1.1
\end{itemize}


\noindent
\textbf{Packages and Dependencies}
\begin{itemize}
    \item Python
    \begin{itemize}
        \item pandas
        \item scipy
        \item matplotlib
        \item seaborn
    \end{itemize}
    \item R
    \begin{itemize}
        \item ggplot2
        \item minpack.lm
    \end{itemize}
\end{itemize}



\section*{Results}
A number of possible metrics to compare the models were considered. Akaike Information Criteria (AIC) and Bayesian Information Criterion (BIC) are two information theory methods which use the estimated log-likelihood of a model for comparison, giving an estimate of the prediction error. Lower values mean that the model is a better fit for the data. A modified version of AIC can be used when sample size is small, named AICc (AIC with \underline{c}orrection for small sample sizes). This method was developed as there is a substantial probability of overfitting using standard AIC with small sample sizes. Overfitting refers to the situation where a model is overly sensitive to the random fluctuations of that specific dataset, and thus has little predictive or diagnostic power. Given the prevalence of small sample sizes in this dataset (only two subsets with $>100$ data points), AICc seemed the most appropriate measure of model performance.

\begin{center}
\def\arraystretch{1.5}
\begin{tabular}{|c|c|}
	\hline
	AIC & $2k - 2\ln(\hat{L})$\\
	BIC & $k\ln(n) - 2\ln(\hat{L})$\\
	AICc & $AIC + \frac{2k^2 + 2k}{n - k - 1}$\\
	\hline
\end{tabular}
\end{center}

In order to compare model performance across the whole dataset, a simple tally was calculated, which counted the number of subsets for which each model was the best performing by AICc. 


\begin{center}
	\includegraphics{../results/tally_bar.png}\\
	Figure 1: Number of subsets for which each model was the best fit by AICc
\end{center}



\section*{Discussion}


\bibliography{../writeup/MiniProject}

\end{document}
